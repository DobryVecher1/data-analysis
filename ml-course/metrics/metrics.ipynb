{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "3fa3a2a7590b3915ab17662d71834416bd3a1b2d99481e19a256b8ec08b40195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Find the confusion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import precision_recall_curve\r\n",
    "\r\n",
    "df_class = pd.read_csv('classification.csv')\r\n",
    "\r\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\r\n",
    "# Create \"sum\" column (different weight in order to tell the 1-0/0-1 cases)\r\n",
    "df_class['sum'] = 2*df_class['true'] + df_class['pred']\r\n",
    "tp = df_class['sum'].loc[df_class['sum'] == 3].count()  \r\n",
    "tn = df_class['sum'].loc[df_class['sum'] == 0].count()\r\n",
    "fn = df_class['sum'].loc[df_class['sum'] == 2].count()\r\n",
    "fp = df_class['sum'].loc[df_class['sum'] == 1].count()\r\n",
    "# Confusion matrix\r\n",
    "conf_mat = pd.DataFrame([[tp, fp], [fn, tn]], index=['a(x) = 1', 'a(x) = 0'], \r\n",
    "                                              columns=['y = 1', 'y = 0'])\r\n",
    "conf_mat.head()\r\n",
    "# Save answer   \r\n",
    "# txt_file = open('conf_mat_ans.txt', 'w')\r\n",
    "# arr2save = [tp, fp, fn, tn]\r\n",
    "# for ii in arr2save:\r\n",
    "#     txt_file.write('%g ' % np.round(ii, 2))\r\n",
    "# txt_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find accuracy metrics of the classificator:\n",
    "$$accuracy = \\frac{TP+TN}{TP+FP+FN+TN} $$\n",
    "$$precision = \\frac{TP}{TP+FP} $$\n",
    "$$recall = \\frac{TP}{TP+FN} $$\n",
    "$$F score = \\frac{2*precision*recall}{precision+recall} $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "accuracy = (tp+tn)/(tp+fp+fn+tn)\r\n",
    "precision = tp/(tp+fp)\r\n",
    "recall = tp/(tp+fn)\r\n",
    "f_score = 2*precision*recall/(precision+recall)\r\n",
    "metrics = pd.Series([accuracy, precision, recall, f_score],\r\n",
    "                    index=['accuracy', 'precision', 'recall', 'f_score'], name='Score')\r\n",
    "metrics.head()\r\n",
    "# Save answer   \r\n",
    "# txt_file = open('metrics_ans.txt', 'w')\r\n",
    "# arr2save = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f_score']]\r\n",
    "# for ii in arr2save:\r\n",
    "#     txt_file.write('%g ' % np.round(ii, 2))\r\n",
    "# txt_file.close()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "accuracy     0.535000\n",
       "precision    0.558442\n",
       "recall       0.421569\n",
       "f_score      0.480447\n",
       "Name: Score, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "scores = pd.read_csv('scores.csv')\n",
    "logreg = roc_auc_score(scores['true'], scores['score_logreg'])\n",
    "svm = roc_auc_score(scores['true'], scores['score_svm'])\n",
    "knn = roc_auc_score(scores['true'], scores['score_knn'])\n",
    "tree = roc_auc_score(scores['true'], scores['score_tree'])\n",
    "print('AUC-ROC scores:\\nlogreg = %g \\nsvm = %g \\nknn = %g \\ntree = %g' % (logreg, svm, knn, tree))\n",
    "# Save answer   \n",
    "# txt_file = open('auc_roc_ans.txt', 'w')\n",
    "# txt_file.write(scores.columns[1])\n",
    "# txt_file.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC-ROC scores:\n",
      "logreg = 0.719188 \n",
      "svm = 0.708683 \n",
      "knn = 0.635154 \n",
      "tree = 0.691927\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def max_prec(pr):\n",
    "    \"\"\"\n",
    "    Find the max value of precision when recal >= 0.7\n",
    "    \"\"\"\n",
    "    precision = pr[0]\n",
    "    recall = pr[1]\n",
    "    max_prec = precision[np.nonzero(recall >= 0.7)].max()\n",
    "\n",
    "    return max_prec\n",
    "\n",
    "pr_logreg = precision_recall_curve(scores['true'], scores['score_logreg'])\n",
    "pr_svm = precision_recall_curve(scores['true'], scores['score_svm'])\n",
    "pr_knn = precision_recall_curve(scores['true'], scores['score_knn'])\n",
    "pr_tree = precision_recall_curve(scores['true'], scores['score_tree'])\n",
    "\n",
    "pr_max_acc = pd.Series([max_prec(pr_logreg), \n",
    "                      max_prec(pr_svm), \n",
    "                      max_prec(pr_knn), \n",
    "                      max_prec(pr_tree),])\n",
    "# Save answer   \n",
    "# txt_file = open('max_prec_ans.txt', 'w')\n",
    "# txt_file.write(scores.columns[pr_max_acc.argmax()+1])\n",
    "# txt_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}