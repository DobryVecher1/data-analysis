{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "3fa3a2a7590b3915ab17662d71834416bd3a1b2d99481e19a256b8ec08b40195"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Find the confusion matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "df_class = pd.read_csv('classification.csv')\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "# Create \"sum\" column (different weight in order to tell the 1-0/0-1 cases)\n",
    "df_class['sum'] = 2*df_class['true'] + df_class['pred']\n",
    "tp = df_class['sum'].loc[df_class['sum'] == 3].count()  \n",
    "tn = df_class['sum'].loc[df_class['sum'] == 0].count()\n",
    "fn = df_class['sum'].loc[df_class['sum'] == 2].count()\n",
    "fp = df_class['sum'].loc[df_class['sum'] == 1].count()\n",
    "# Confusion matrix\n",
    "conf_mat = pd.DataFrame([[tp, fp], [fn, tn]], index=['a(x) = 1', 'a(x) = 0'], \n",
    "                                              columns=['y = 1', 'y = 0'])\n",
    "conf_mat.head()\n",
    "# Save answer   \n",
    "txt_file = open('conf_mat_ans.txt', 'w')\n",
    "arr2save = [tp, fp, fn, tn]\n",
    "for ii in arr2save:\n",
    "    txt_file.write('%g ' % np.round(ii, 2))\n",
    "txt_file.close()"
   ]
  },
  {
   "source": [
    "Find accuracy metrics of the classificator:\n",
    "$$accuracy = \\frac{TP+TN}{TP+FP+FN+TN} $$\n",
    "$$precision = \\frac{TP}{TP+FP} $$\n",
    "$$recall = \\frac{TP}{TP+FN} $$\n",
    "$$F score = \\frac{2*precision*recall}{precision+recall} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2*precision*recall/(precision+recall)\n",
    "metrics = pd.Series([accuracy, precision, recall, f_score],\n",
    "                    index=['accuracy', 'precision', 'recall', 'f_score'], name='Score')\n",
    "metrics.head()\n",
    "# Save answer   \n",
    "txt_file = open('metrics_ans.txt', 'w')\n",
    "arr2save = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f_score']]\n",
    "for ii in arr2save:\n",
    "    txt_file.write('%g ' % np.round(ii, 2))\n",
    "txt_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC-ROC scores:\nlogreg = 0.719188 \nsvm = 0.708683 \nknn = 0.635154 \ntree = 0.691927\n"
     ]
    }
   ],
   "source": [
    "scores = pd.read_csv('scores.csv')\n",
    "logreg = roc_auc_score(scores['true'], scores['score_logreg'])\n",
    "svm = roc_auc_score(scores['true'], scores['score_svm'])\n",
    "knn = roc_auc_score(scores['true'], scores['score_knn'])\n",
    "tree = roc_auc_score(scores['true'], scores['score_tree'])\n",
    "print('AUC-ROC scores:\\nlogreg = %g \\nsvm = %g \\nknn = %g \\ntree = %g' % (logreg, svm, knn, tree))\n",
    "# Save answer   \n",
    "txt_file = open('auc_roc_ans.txt', 'w')\n",
    "txt_file.write(scores.columns[1])\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_prec(pr):\n",
    "    \"\"\"\n",
    "    Find the max value of precision when recal >= 0.7\n",
    "    \"\"\"\n",
    "    precision = pr[0]\n",
    "    recall = pr[1]\n",
    "    max_prec = precision[np.nonzero(recall >= 0.7)].max()\n",
    "\n",
    "    return max_prec\n",
    "\n",
    "pr_logreg = precision_recall_curve(scores['true'], scores['score_logreg'])\n",
    "pr_svm = precision_recall_curve(scores['true'], scores['score_svm'])\n",
    "pr_knn = precision_recall_curve(scores['true'], scores['score_knn'])\n",
    "pr_tree = precision_recall_curve(scores['true'], scores['score_tree'])\n",
    "\n",
    "pr_max_acc = pd.Series([max_prec(pr_logreg), \n",
    "                      max_prec(pr_svm), \n",
    "                      max_prec(pr_knn), \n",
    "                      max_prec(pr_tree),])\n",
    "# Save answer   \n",
    "txt_file = open('max_prec_ans.txt', 'w')\n",
    "txt_file.write(scores.columns[pr_max_acc.argmax()+1])\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}